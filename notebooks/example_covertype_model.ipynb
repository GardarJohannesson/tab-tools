{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model demo using the UCI `covertype` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 11 15:42:11 PDT 2024\n"
     ]
    }
   ],
   "source": [
    "# Last run.\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and prepare the data\n",
    "\n",
    "Fetch the `covertype` data from UCI, split and preprocess, and create a data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_ucirepo(\"Covertype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([data.data.features, data.data.targets], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 581012 entries, 0 to 581011\n",
      "Data columns (total 55 columns):\n",
      " #   Column                              Non-Null Count   Dtype\n",
      "---  ------                              --------------   -----\n",
      " 0   Elevation                           581012 non-null  int64\n",
      " 1   Aspect                              581012 non-null  int64\n",
      " 2   Slope                               581012 non-null  int64\n",
      " 3   Horizontal_Distance_To_Hydrology    581012 non-null  int64\n",
      " 4   Vertical_Distance_To_Hydrology      581012 non-null  int64\n",
      " 5   Horizontal_Distance_To_Roadways     581012 non-null  int64\n",
      " 6   Hillshade_9am                       581012 non-null  int64\n",
      " 7   Hillshade_Noon                      581012 non-null  int64\n",
      " 8   Hillshade_3pm                       581012 non-null  int64\n",
      " 9   Horizontal_Distance_To_Fire_Points  581012 non-null  int64\n",
      " 10  Wilderness_Area1                    581012 non-null  int64\n",
      " 11  Soil_Type1                          581012 non-null  int64\n",
      " 12  Soil_Type2                          581012 non-null  int64\n",
      " 13  Soil_Type3                          581012 non-null  int64\n",
      " 14  Soil_Type4                          581012 non-null  int64\n",
      " 15  Soil_Type5                          581012 non-null  int64\n",
      " 16  Soil_Type6                          581012 non-null  int64\n",
      " 17  Soil_Type7                          581012 non-null  int64\n",
      " 18  Soil_Type8                          581012 non-null  int64\n",
      " 19  Soil_Type9                          581012 non-null  int64\n",
      " 20  Soil_Type10                         581012 non-null  int64\n",
      " 21  Soil_Type11                         581012 non-null  int64\n",
      " 22  Soil_Type12                         581012 non-null  int64\n",
      " 23  Soil_Type13                         581012 non-null  int64\n",
      " 24  Soil_Type14                         581012 non-null  int64\n",
      " 25  Soil_Type15                         581012 non-null  int64\n",
      " 26  Soil_Type16                         581012 non-null  int64\n",
      " 27  Soil_Type17                         581012 non-null  int64\n",
      " 28  Soil_Type18                         581012 non-null  int64\n",
      " 29  Soil_Type19                         581012 non-null  int64\n",
      " 30  Soil_Type20                         581012 non-null  int64\n",
      " 31  Soil_Type21                         581012 non-null  int64\n",
      " 32  Soil_Type22                         581012 non-null  int64\n",
      " 33  Soil_Type23                         581012 non-null  int64\n",
      " 34  Soil_Type24                         581012 non-null  int64\n",
      " 35  Soil_Type25                         581012 non-null  int64\n",
      " 36  Soil_Type26                         581012 non-null  int64\n",
      " 37  Soil_Type27                         581012 non-null  int64\n",
      " 38  Soil_Type28                         581012 non-null  int64\n",
      " 39  Soil_Type29                         581012 non-null  int64\n",
      " 40  Soil_Type30                         581012 non-null  int64\n",
      " 41  Soil_Type31                         581012 non-null  int64\n",
      " 42  Soil_Type32                         581012 non-null  int64\n",
      " 43  Soil_Type33                         581012 non-null  int64\n",
      " 44  Soil_Type34                         581012 non-null  int64\n",
      " 45  Soil_Type35                         581012 non-null  int64\n",
      " 46  Soil_Type36                         581012 non-null  int64\n",
      " 47  Soil_Type37                         581012 non-null  int64\n",
      " 48  Soil_Type38                         581012 non-null  int64\n",
      " 49  Soil_Type39                         581012 non-null  int64\n",
      " 50  Soil_Type40                         581012 non-null  int64\n",
      " 51  Wilderness_Area2                    581012 non-null  int64\n",
      " 52  Wilderness_Area3                    581012 non-null  int64\n",
      " 53  Wilderness_Area4                    581012 non-null  int64\n",
      " 54  Cover_Type                          581012 non-null  int64\n",
      "dtypes: int64(55)\n",
      "memory usage: 243.8 MB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wilderness_Area1\n",
       "0    320216\n",
       "1    260796\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"Wilderness_Area1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    \"target\": \"Cover_Type\",\n",
    "    \"task\": \"classification\",\n",
    "    # Numerical columns that will be normalized.\n",
    "    \"numerical_columns_norm\": [\n",
    "        \"Elevation\",\n",
    "        \"Aspect\",\n",
    "        \"Slope\",\n",
    "        \"Horizontal_Distance_To_Hydrology\",\n",
    "        \"Vertical_Distance_To_Hydrology\",\n",
    "        \"Horizontal_Distance_To_Roadways\",\n",
    "        \"Hillshade_9am\",\n",
    "        \"Hillshade_Noon\",\n",
    "        \"Hillshade_3pm\",\n",
    "        \"Horizontal_Distance_To_Fire_Points\",\n",
    "    ],\n",
    "    # Numerical non-transformed (raw) columns.\n",
    "    \"numerical_columns_raw\": [\n",
    "        \"Wilderness_Area1\",\n",
    "        \"Wilderness_Area2\",\n",
    "        \"Wilderness_Area3\",\n",
    "        \"Wilderness_Area4\",\n",
    "        \"Soil_Type1\",\n",
    "        \"Soil_Type2\",\n",
    "        \"Soil_Type3\",\n",
    "        \"Soil_Type4\",\n",
    "        \"Soil_Type5\",\n",
    "        \"Soil_Type6\",\n",
    "        \"Soil_Type7\",\n",
    "        \"Soil_Type8\",\n",
    "        \"Soil_Type9\",\n",
    "        \"Soil_Type10\",\n",
    "        \"Soil_Type11\",\n",
    "        \"Soil_Type12\",\n",
    "        \"Soil_Type13\",\n",
    "        \"Soil_Type14\",\n",
    "        \"Soil_Type15\",\n",
    "        \"Soil_Type16\",\n",
    "        \"Soil_Type17\",\n",
    "        \"Soil_Type18\",\n",
    "        \"Soil_Type19\",\n",
    "        \"Soil_Type20\",\n",
    "        \"Soil_Type21\",\n",
    "        \"Soil_Type22\",\n",
    "        \"Soil_Type23\",\n",
    "        \"Soil_Type24\",\n",
    "        \"Soil_Type25\",\n",
    "        \"Soil_Type26\",\n",
    "        \"Soil_Type27\",\n",
    "        \"Soil_Type28\",\n",
    "        \"Soil_Type29\",\n",
    "        \"Soil_Type30\",\n",
    "        \"Soil_Type31\",\n",
    "        \"Soil_Type32\",\n",
    "        \"Soil_Type33\",\n",
    "        \"Soil_Type34\",\n",
    "        \"Soil_Type35\",\n",
    "        \"Soil_Type36\",\n",
    "        \"Soil_Type37\",\n",
    "        \"Soil_Type38\",\n",
    "        \"Soil_Type39\",\n",
    "        \"Soil_Type40\",\n",
    "    ],\n",
    "    # Categorical columns that will be embedded.\n",
    "    \"categorical_columns_embed\": [],\n",
    "    # Categorical columns that will be one-hot encoded.\n",
    "    \"categorical_columns_onehot\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabDataset(Dataset):\n",
    "    \"\"\"Tabular dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_df (pd.DataFrame): Dataframe containing the data.\n",
    "        config (dict): Configuration dictionary.\n",
    "        transform (callable): Optional transform to be applied on a sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_df: pd.DataFrame, config: dict, transform=None):\n",
    "        self.data_size = len(data_df)\n",
    "        # All numerical features to be normalized.\n",
    "        self.numerical_features_norm = data_df[\n",
    "            config[\"numerical_columns_norm\"]\n",
    "        ].values.astype(\"float32\")\n",
    "        # All numerical features that are not normalized.\n",
    "        self.numerical_features_raw = data_df[\n",
    "            config[\"numerical_columns_raw\"]\n",
    "        ].values.astype(\"float32\")\n",
    "        # Categorical features that will be embedded.\n",
    "        self.categorical_features_embed = data_df[\n",
    "            config[\"categorical_columns_embed\"]\n",
    "        ].values.astype(\"int64\")\n",
    "        # Categorical features that will be one-hot encoded.\n",
    "        self.categorical_features_onehot = data_df[\n",
    "            config[\"categorical_columns_onehot\"]\n",
    "        ].values.astype(\"int64\")\n",
    "        # Target variable.\n",
    "        target_type = \"int64\" if config[\"task\"] == \"classification\" else \"float32\"\n",
    "        self.target = (\n",
    "            data_df[config[\"target\"]].values.astype(target_type)\n",
    "        )\n",
    "        self.config = config\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, idx: List[int]) -> dict:\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        X_norm = self.numerical_features_norm[idx]\n",
    "        X_raw = self.numerical_features_raw[idx]\n",
    "        X_embed = self.categorical_features_embed[idx]\n",
    "        X_onehot = self.categorical_features_onehot[idx]\n",
    "        y = self.target[idx]\n",
    "        data = {\n",
    "            \"X_norm\": X_norm,\n",
    "            \"X_raw\": X_raw,\n",
    "            \"X_embed\": X_embed,\n",
    "            \"X_onehot\": X_onehot,\n",
    "            \"y\": y,\n",
    "        }\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def data(self) -> pd.DataFrame:\n",
    "        \"\"\"A DataFrame containing the data.\"\"\"\n",
    "        return pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(\n",
    "                    self.numerical_features_norm,\n",
    "                    columns=self.config[\"numerical_columns_norm\"],\n",
    "                ),\n",
    "                pd.DataFrame(\n",
    "                    self.numerical_features_raw,\n",
    "                    columns=self.config[\"numerical_columns_raw\"],\n",
    "                ),\n",
    "                pd.DataFrame(\n",
    "                    self.categorical_features_embed,\n",
    "                    columns=self.config[\"categorical_columns_embed\"],\n",
    "                ),\n",
    "                pd.DataFrame(\n",
    "                    self.categorical_features_onehot,\n",
    "                    columns=self.config[\"categorical_columns_onehot\"],\n",
    "                ),\n",
    "                pd.DataFrame(self.target, columns=[self.config[\"target\"]]),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "\n",
    "class TabDataTransformer:\n",
    "    \"\"\"Transform the data given by the TabDataset.\n",
    "\n",
    "    This includes normalize numerical features and for categorical features, either one-hot encode or apply\n",
    "    ordinal encoding (in preperaton for creating embeddings). In addition, apply label encoding for the target\n",
    "    variable if it is a classification task.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary.\n",
    "\n",
    "    Returns:\n",
    "        dict: Transformed data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        if config[\"task\"] == \"classification\":\n",
    "            self.label_encoder = LabelEncoder()\n",
    "        self.scaler = self._scaler()\n",
    "        # Keep track of encoders for each categorical column.\n",
    "        self.one_hod_encoders = {cn: self._one_hod_encoder() for cn in config[\"categorical_columns_onehot\"]}\n",
    "        self.embed_encoders = {cn: self._embed_encoder() for cn in config[\"categorical_columns_embed\"]}\n",
    "\n",
    "    def _one_hod_encoder():\n",
    "        \"\"\"For the purpose of cusomization.\"\"\"\n",
    "        return OneHotEncoder()\n",
    "    \n",
    "    def _scaler(self):\n",
    "        \"\"\"For the purpose of cusomization.\"\"\"\n",
    "        return RobustScaler()\n",
    "    \n",
    "    def _embed_encoder(self):\n",
    "        \"\"\"For the purpose of cusomization.\"\"\"\n",
    "        return OrdinalEncoder()\n",
    "\n",
    "    def _fit_scaler(self, X: np.ndarray):\n",
    "        self.scaler.fit(X)\n",
    "\n",
    "    def _fit_label_encoders(self, y: np.ndarray):\n",
    "        self.label_encoder.fit(y)\n",
    "\n",
    "    def _fit_one_hot_encoders(self, X: np.ndarray):\n",
    "        for i, cn in enumerate(self.config[\"categorical_columns_onehot\"]):\n",
    "            self.one_hod_encoders[cn].fit(X[:, i].reshape(-1, 1))\n",
    "\n",
    "    def _fit_embed_encoders(self, X: np.ndarray):\n",
    "        for i, cn in enumerate(self.config[\"categorical_columns_embed\"]):\n",
    "            self.embed_encoders[cn].fit(X[:, i].reshape(-1, 1))\n",
    "\n",
    "    def fit(self, data: TabDataset):\n",
    "        self._fit_scaler(data.numerical_features_norm)\n",
    "        self._fit_label_encoders(data.target)\n",
    "        self._fit_one_hot_encoders(data.categorical_features_onehot)\n",
    "        self._fit_embed_encoders(data.categorical_features_embed)\n",
    "\n",
    "    def transform(self, data: dict):\n",
    "        X_norm = self.scaler.transform(data[\"X_norm\"])\n",
    "        X_onehot = np.hstack(\n",
    "            [\n",
    "                self.one_hod_encoders[cn].transform(data[\"X_onehot\"][:, i].reshape(-1, 1)).toarray()\n",
    "                for i, cn in enumerate(self.config[\"categorical_columns_onehot\"])\n",
    "            ]\n",
    "        )\n",
    "        X_embed = np.hstack(\n",
    "            [\n",
    "                self.embed_encoders[cn].transform(data[\"X_embed\"][:, i].reshape(-1, 1))\n",
    "                for i, cn in enumerate(self.config[\"categorical_columns_embed\"])\n",
    "            ]\n",
    "        )\n",
    "        y = self.label_encoder.transform(data[\"y\"])\n",
    "        return {\n",
    "            \"X_norm\": X_norm,\n",
    "            \"X_onehot\": X_onehot,\n",
    "            \"X_embed\": X_embed,\n",
    "            \"y\": y,\n",
    "        }\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_ds = TabDataset(data_df.sample(100), data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_norm': array([[3035.,   91.,   13.,  182.,   36., 5155.,  240.,  219.,  108.,\n",
       "         3302.],\n",
       "        [2110.,  349.,   21.,  319.,    0.,  417.,  177.,  201.,  159.,\n",
       "          685.]], dtype=float32),\n",
       " 'X_raw': array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
       " 'X_embed': array([], shape=(2, 0), dtype=int64),\n",
       " 'X_onehot': array([], shape=(2, 0), dtype=int64),\n",
       " 'y': array([2, 3])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_ds[[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2905060 entries, 0 to 581011\n",
      "Data columns (total 55 columns):\n",
      " #   Column                              Dtype  \n",
      "---  ------                              -----  \n",
      " 0   Elevation                           float32\n",
      " 1   Aspect                              float32\n",
      " 2   Slope                               float32\n",
      " 3   Horizontal_Distance_To_Hydrology    float32\n",
      " 4   Vertical_Distance_To_Hydrology      float32\n",
      " 5   Horizontal_Distance_To_Roadways     float32\n",
      " 6   Hillshade_9am                       float32\n",
      " 7   Hillshade_Noon                      float32\n",
      " 8   Hillshade_3pm                       float32\n",
      " 9   Horizontal_Distance_To_Fire_Points  float32\n",
      " 10  Wilderness_Area1                    float32\n",
      " 11  Wilderness_Area2                    float32\n",
      " 12  Wilderness_Area3                    float32\n",
      " 13  Wilderness_Area4                    float32\n",
      " 14  Soil_Type1                          float32\n",
      " 15  Soil_Type2                          float32\n",
      " 16  Soil_Type3                          float32\n",
      " 17  Soil_Type4                          float32\n",
      " 18  Soil_Type5                          float32\n",
      " 19  Soil_Type6                          float32\n",
      " 20  Soil_Type7                          float32\n",
      " 21  Soil_Type8                          float32\n",
      " 22  Soil_Type9                          float32\n",
      " 23  Soil_Type10                         float32\n",
      " 24  Soil_Type11                         float32\n",
      " 25  Soil_Type12                         float32\n",
      " 26  Soil_Type13                         float32\n",
      " 27  Soil_Type14                         float32\n",
      " 28  Soil_Type15                         float32\n",
      " 29  Soil_Type16                         float32\n",
      " 30  Soil_Type17                         float32\n",
      " 31  Soil_Type18                         float32\n",
      " 32  Soil_Type19                         float32\n",
      " 33  Soil_Type20                         float32\n",
      " 34  Soil_Type21                         float32\n",
      " 35  Soil_Type22                         float32\n",
      " 36  Soil_Type23                         float32\n",
      " 37  Soil_Type24                         float32\n",
      " 38  Soil_Type25                         float32\n",
      " 39  Soil_Type26                         float32\n",
      " 40  Soil_Type27                         float32\n",
      " 41  Soil_Type28                         float32\n",
      " 42  Soil_Type29                         float32\n",
      " 43  Soil_Type30                         float32\n",
      " 44  Soil_Type31                         float32\n",
      " 45  Soil_Type32                         float32\n",
      " 46  Soil_Type33                         float32\n",
      " 47  Soil_Type34                         float32\n",
      " 48  Soil_Type35                         float32\n",
      " 49  Soil_Type36                         float32\n",
      " 50  Soil_Type37                         float32\n",
      " 51  Soil_Type38                         float32\n",
      " 52  Soil_Type39                         float32\n",
      " 53  Soil_Type40                         float32\n",
      " 54  Cover_Type                          float64\n",
      "dtypes: float32(54), float64(1)\n",
      "memory usage: 642.8 MB\n"
     ]
    }
   ],
   "source": [
    "uci_dataset.data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TabDataTransformer(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.fit(tmp_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 140\u001b[0m, in \u001b[0;36mTabDataTransformer.transform\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    139\u001b[0m     X_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mtransform(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 140\u001b[0m     X_onehot \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hod_encoders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX_onehot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategorical_columns_onehot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     X_embed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack(\n\u001b[1;32m    147\u001b[0m         [\n\u001b[1;32m    148\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_encoders[cn]\u001b[38;5;241m.\u001b[39mtransform(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_embed\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, i]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    149\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i, cn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_columns_embed\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    150\u001b[0m         ]\n\u001b[1;32m    151\u001b[0m     )\n\u001b[1;32m    152\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_encoder\u001b[38;5;241m.\u001b[39mtransform(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/tab-tools-IGMAC-dC-py3.12/lib/python3.12/site-packages/numpy/core/shape_base.py:359\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "transformer.transform(tmp_ds[[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tab-tools-IGMAC-dC-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
